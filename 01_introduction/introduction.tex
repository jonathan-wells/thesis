\documentclass[a4paper,11pt,twoside,openright]{scrbook}

\usepackage{../jnwthesis}
\usepackage{lipsum}
\usepackage{standalone}
\standalonetrue

% Bibliography
% \usepackage[backend=biber,style=nature,backref=true]{biblatex}
\bibliography{/Users/jonwells/Documents/bibtex/Thesis}

% Figures
% \usepackage{graphicx}
\graphicspath{ {../figs/wip/} }

\begin{document}


\chapter{Introduction}

\section{What are protein complexes?}
At the heart of Earth's biosphere there lies an ancient, universally conserved metabolic network, comprising just a handful of organic molecules, catalysts and cofactors. This network, found in various forms wherever life exists, channels solar and chemical energy into thermal energy, by way of biological matter. Today, the planetary energy flux that runs through this network is only possible due to the existence of proteins - large oligomers which drive the constituent reactions. Without proteins, biochemistry would be limited to small, autocatalytic cycles occurring at the interfaces between rock, water and sky \cite{Smith2016}; put simply, proteins make life possible.

% TODO Improve and tighten up section about complexity. Check your maths! If not 21 then need to correct this for all subsequent calcs too (and in footnote).
Each protein is comprised of tens to thousands of amino acids, linked by covalent peptide bonds between terminal amine and carboxyl groups. There are 20 amino acids that are commonly used by biological organisms, meaning that an N residue sequence can be composed in \(20^{N}\) different ways - for a typical 200 residue protein this gives \(1.6 \times 10^{260}\) possible sequence arrangements. On top of this, the function of a protein is critically dependent on its three dimensional structure, as defined by the bond angles between every pair of residues. As a result of this combination of sequence and structural variability, the theoretical complexity of the protein universe is essentially limitless. What is more, there are indications that actual - non-theoretical - sequence space is growing essentially randomly; despite the constraints on individual proteins, over the long term common protein functions (if not folds) can exist even whilst sequence similarity approaches that of random sequences \cite{Larson2002,Povolotskaya2010}.

% NOTE: were the first proteins enzymes then? Again, Gyorgy's stuff on protein

% NOTE *Define* homomers and heteromers - introduce them more explicitly. Are most proteins enzymes or not?
Even beyond the complexity inherent in each amino acid chain, proteins interact extensively with one another and their surrounding environment. Indeed, the function of a protein is usually stated in terms of its interactions with other biological molecules. In the case of enzymes, these interactors are typically small metabolites; however, since most proteins are not enzymes, the majority of biologically interesting interactions are simply with other proteins. Most of these protein-protein interactions are fleeting, arising as a result of dense intracellular crowding, and may or may not be functional. Many though last longer, and produce stable protein complexes, the formation of which is generally essential to the proper function of the constituent protein subunits. It is the formation of these protein complexes that is the main topic of this work.

Before discussing these matters further however, we first need to define some terms. With respect to the quaternary structure of a protein, there are three overarching classes that encompass all possible structures. A single protein chain that mostly exists without forming stable interactions with other proteins is known as a monomer. More common however, is the case where interactions occur between identical copies of a single protein species; in this case the resulting complex is known as a homomer. Finally, heteromers are formed from groups of distinct, non-identical proteins. In addition to these three categories, we can also talk about features such as the stoichiometry of the complex - that is, the numbers and ratios of different subunits that make up the whole. Likewise, we can also separate complexes into symmetry groups. To put this into practice, we can use the example of a tetrameric dihedral homomer; this is a concise way of describing a protein with four identical copies of a single protein chain, arranged in a dimer of dimers, with three axes of symmetry.

% NOTE: Point out that the core metabolic network and the protein universe are deeply intertwined - read up about evolution of cofactors. Gyorgy's ligand binding paper would be good here, especially if you can hint at the link between diversification of protein species and the metabolic network. Symmetry breaking or something like that, in that perhaps multichain binding sites in homomers could lead to heteromers. THIS IS (going to be) A GOOD PARAGRAPH, keep it but move till after you have defined homomers, heteromers etc. Why and how did protein complexes evolve, discuss Gyorgy's work, then link this into Lynch's non-adaptive arguments.

% TODO Paragraph on what proteins do, and why they do things that can't be done by monomeric proteins. Point out that Lynch's argument applies to homomeric proteins, but it is probably less applicable to heteromers
\subsubsection{The evolution of protein complexes}
If stable protein complexes are so common, then what (if any) evolutionary fitness benefit do they confer on the cell? One important hypothesis is the non-adaptive one, such as that proposed by Michael Lynch in 2013 \cite{Lynch2013}. In this important paper he first notes the extensive literature showing how homomeric interfaces can be formed or destroyed by simple point mutations in the protein of interest, and the fact that proteins are frequently found in different multimeric states in different organisms. Armed with these observations, he describes a simple model in which transitions between multimeric states are represented as a Markov process, the probabilities of which are dependent on the underlying mutation rate and selection pressure. The implication arising from this is that under neutral to modest selection pressure (which is the case for the large majority of eukaryotic genomes), mutations that promote homomerisation of a given protein will arise as a consequence of genetic drift.
% NOTE does this lead to a gamma distribution? can I show that this fits the observed distribution of proteins in the PDB?

% TODO New paragraph, what are the main advantages - allostery, particularly in homomers, reference Monod allostery paper (1st citation in Lynch2013). For heteromers, it perhaps brings together different functional units, e.g metabolic pathways. A facile example could be actin and myosin, which obviously need something to lever against to generate force. Can you ask Gyorgy about mentioning his results in your thesis?
The fact that non-adaptive explanations can often be sufficient to explain seemingly complex biological phenomena is an important one, and provides an essential null hypothesis that should be addressed before turning to adaptive (Darwinian) explanations \cite{Koonin2016}. Similarly, exaptation, the process by which new functions are generated from redundant or no longer necessary parts \cite{Gould1979}. Undoubtedly, however all of the above contribute in some degrees to the the ubiquity of protein complexes to varying degrees.

% NOTE: ugly last sentence, needs improving.
There are numerous potentially adaptive benefits provided by the formation of protein complexes. For example, when considering the metabolic cost of synthesising proteins, it may be more effective to split a large protein into parts, so that errors in translation are restricted to smaller units - modularity is a key feature of most robust systems, whether biological or designed. This is pertinent since the error rates in gene expression are sufficiently high as to present a major challenge to the viability of life in general. The literature on this topic is extensive, but for a good review focussing on error control in translation see Gingold and Pilpel, 2011 \cite{Gingold2011}. For more spectacular examples of situations in which formation of complexes has enabled novel evolutionary solutions, one only has to look at ATP synthase, which, being a fully functional rotary motor, could not possibly exist in its current state were it formed from a single polypeptide chain.

% NOTE Read the Monod paper and use their definition, not some crackpot one you've made up. Also, consider moving the allostery paragraph to a slighrtly later section, since its kinda interesting in that if allostery was such a clear adaptive benefit then you might expect precise allosteric mechanisms to be conserved - they are not always! Can you cite Joe and Tee's deer project here?
Another potentially adaptive and particularly widespread phenomena arising from the formation of protein complexes is that of allostery. The original definition of allostery given by Monod, Changeux and Jacob \cite{Monod1963} referred to modulation of protein activity by small molecules binding away from the active site, but this definition has since been extended to include cooperative effects between proteins. The classic example of this is of course haemoglobin, in which the binding of oxygen to one subunit increases the binding affinity of neighbouring subunits by the propagation of structural changes through the subunit binding interfaces \cite{Perutz1976}. Intriguingly however, though one might expect beneficial allosteric mechansims to be highly conserved, it turns out that the opposite is frequently true; for example, the extensively studied mechanisms of haemoglobin is known to differ mechanistically in different species \cite{Royer2005,Bellelli2011}. This begs the question: to what degree is allostery an inevitable consequence of?

% TODO What are the aims of this thesis? Why does assembly matter? Needs improving, is an important paragraph. Maybe look for a more
Thus far we have discussed protein complexes as fully formed entities. However, as we shall see over the course of this work, the assembly process itself has great biological importance, and many phenomena can be better understood by taking it into account - the attenuation of protein levels observed in aneuploid cells (chapter 5) being a notable example. A central fact that underlies most of the work in this thesis is that the process by which protein complexes assemble is stochastic. However, whilst the binding of any two subunits is random - a product of Brownian motion and binding affinity - in aggregate, the assembly of protein complexes proceeds via non-random, ordered pathways \cite{Levy2008a, Marsh2013}. This fact has profound implications for the behaviour of the cell.

% NOTE: IMPORTANT: Not sure quite how to lead into the review section, but possibly the historical overview would be nice. Important part is: "The following review is intended to give the reader a broad picture of the methods currently available for the study of protein complexes. The focus/emphasis? is not on methodological details, but rather on TK (design?) considerations and in how they enable different pictures of the protein world, not just at the narrow definition of individual protein complexes, but with a view to understanding how they fit into the larger proteome. Without these experimental advances, the work presented in the rest of the thesis would have been unthinkable." Then maybe a footnote about Donald rumsfeld's known unknowns.

% NOTE: need to mention Dorothy Hodgkin
\subsubsection{A brief history of research on protein complexes}
The tendency of proteins to form complexes and the functional implications of this behaviour has been recognised since the earliest days of molecular biology. Though it is unclear who was the first to explicitly note their existence, it seems likely that interest in protein complexes arose in tandem with investigations into the nature of viruses. In 1935, W. M. Stanley reported the isolation of `a crystalline material which has the properties of tobacco-mosaic virus' (TMV), and demonstrated that this material was predominantly composed of protein \cite{Stanley1935}. However, it is not obvious that he thought of it as simply being an unusually large protein complex in the sense that we might, or of the implications of his discovery for proteins beyond those comprising the TMV capsid. Nonetheless, this period in time marks a turning point for the field of biology, and TK (removed big changes were soon to come.).

%NOTE: improve last sentence
As if to usher in the era, 1944 saw the publication of Erwin Schrödinger's classic book: `What is Life?' \cite{Schrodinger1947}, which inspired a number of scientists, particularly physicists, to try their hand at biology. Amongst these were names such as Francis Crick, James Watson and Maurice Wilkins, who were are best known for their discovery in 19XX of the structure of DNA\footnote{Rosalind Franklin of course was infamously snubbed by Watson and Crick, who only grudgingly gave her credit for her essential work producing the essential X-ray diffraction patterns that were used to help solve the structure. Franklin was also involved in pioneering work using crystallographic electron microscopy to investigate the structure of viruses. Sadly, she died in 1958 at the age of 37, before achieving the recognition she deserved, but this later work eventually led to her proteg{\'e},  Aaron Klug, winning the 1982 Nobel prize for Chemistry. It seems possible that, had she lived, Franklin would have been in line for two Nobels, so maybe she gets the last laugh in the end.}. Also familiar with the book, though apparently not so enamoured with it \cite{Dronamraju1999}, was Max Perutz, who was at the time working on haemoglobin. By this point, it was clear that many proteins were multimeric assemblies, and already in 1955 the TMV capsid had been described as a self-assembling homomer comprised of several thousand identical subunits \cite{Fraenkel-Conrat1955}. All that remained was for the first structures to be solved, a feat that was achieved before the end of the decade by John Kendrew and Max Perutz - first with monomeric Myoglobin \cite{Kendrew1958}, and shortly thereafter tetrameric Haemoglobin \cite{Perutz1960}. These structures opened up the door to the new field of structural biology.

% NOTE: what was happening during the war? ribosome citations please. Continue this with a discussion of what was going on with other methods at this time, refer to the amusing nature of that timeline paper \cite{Campbell2002} wrt how out of date it sounds.
Following the first post-war forays into the characterisation of protein complexes, technology improved rapidly, and during this period the field of structural biology was (and to this day still is) one of the most productive in all of science; X-ray crystallography in particular deserves special mention, having lead to no fewer than 14 Nobel Prizes since 1914. Of these Nobels, uncovering the structure of the ribosome - a huge complex consisting of dozens of protein and rRNA subunits - is perhaps the crowning achievement \cite{Schluenzen2000,Ramakrishnan2000,Ban2000}.

% NOTE: finish with ESI and bring up to date. Also, the very last sentence is not super correct. ESI is native, and LC-MS is more different, for proteomics not individiual complexes.
In the 20 years leading up to this achievement, other fields had not been silent, and mass spectrometry was already an established technique by the time x-ray crystallography arrived on the scene. The analysis of proteins with mass spectrometry did not arrive until the mid '80s however, with the development of soft matrix-assisted laser desorption/ionization (MALDI) and related techniques by Karas, Bachmann, Hillenkamp and Tanaka \cite{Karas1985,Tanaka1988}. Almost simultaneously, a second soft ionisation process - electrospray ionisation (ESI) - was enabled for use with proteins \cite{Fenn1989}. The latter of these is now widely used for high-throughout proteomics and via coupling with liquid chromatography and affinity purification has made it possible to infer the existence of protein complexes from large scale pairwise interaction data.

AMBER \cite{Salomon-Ferrer2013,Amber2017} (Assisted Model Building and Energy Refinement) and CHARMM \cite{Brooks2009} (Chemistry at HARvard Molecular Mechanics)

Computational biology, launched in the '60s by the pioneering and remarkably prescient efforts of researchers such as Margaret Dayhoff and Russel Doolittle, has now developed into a mature field capable of tackling diverse problems. These first steps in the field were predominantly concerned with producing tools for sequence anaylsis (at a time when just 65 proteins had been sequenced! \cite{Dayhoff1965}), but shortly after this work began on the first force fields for modelling chemical structures \cite{Bixon1967}. Michael Levitt began trying to apply these early force fields to the problem of protein structure in 1968 in John Kendrew's lab at the MRC Laboratory for Molecular Biology \cite{Levitt2001} and eventually this work - along with that of others in the community - led to the development of CHARMM \cite{Brooks2009} (Chemistry at HARvard Molecular Mechanics) and AMBER \cite{Salomon-Ferrer2013,Amber2017} (Assisted Model Building and Energy Refinement), both of which are still in use today.

However, molecular dynamics, as the field is known, is very computationally intensive, and is only now becoming feasible for protein complexes. At a time when computers were not yet powerful enough to model large proteins, the number of sequences available to researchers was beginning to explode, and with this wealth of data the technique of homology modelling became possible \cite{Rost1996,Xu1998}. This has had a huge impact on our ability to predict structures, but also significantly speeds up x-ray crystallography, since it makes it possible to to brute force the phase problem for proteins with known sequence homology to existing structures.

% NOTE: swap round cryo em and x-ray sections?
\section{Structural characterisation of protein complexes}
Whilst X-ray crystallography of most soluble monomeric proteins is now routine, there are still intractable difficulties for membrane proteins and large complexes. Recently though, cryo-EM has seen a dramatic resurgence after a long hiatus, driven by combined improvements in electron detectors and image processing approaches. Crucially, since it does not rely on being able to crystallise the sample of interest, cryo-EM is able to fill in many of the gaps left by diffucult-to-crystallise proteins. Through the aid of computational modelling, these methods are now beginning to be combined, with X-ray crystallography providing atomic resolution details of individual subunits and cryo-EM the coarse-grained structure of the larger complexes.

\subsection{X-ray crystallography}
% NOTE: For many years, the structural biology was synonoymous with X-ray crystallography, and certainly X-ray crystallography is the method that first made the field a reality.
X-ray crystallography was the first method to make the field of structural biology a reality, and brought together three key technologies, each important in its own right. These technologies are: the ability to overexpress and purify proteins, the development of powerful sources of X-rays, and computational methods for resolving the diffraction patterns produced by firing X-rays at purified protein crystals. By and large, the ways in which X-ray crystallography can be used to determine protein structure are the same for monomeric proteins and those which form complexes. There are however some important differences and additional difficulties that are worth considering. Furthermore, although cryo-EM is poised to overtake X-ray crystallography as the method of choice for the solution of large heteromeric structures, there have been a number of exciting developments in the latter field that look set to ensure its future for many years to come. In the following section I will highlight of some of these advances, and attempt to give a summary of the current state of the field.

% NOTE: Are you just repearting yourself here with the AP-MS stuff?
\subsubsection{Protein expression, purification, and crystallisation}
% CITE: difficulty in overexpressing protein complexes.
A generic protocol for the expression of a protein for cystallisation would involve transforming \textit{E. coli} with a plasmid containing your protein of interest, typically under the control of a strong, inducible promotor. Such a protocol works well for monomeric bacterial proteins, but expressing heteromeric protein complexes is often significantly more challenging. The key difficulty in the expression of heteromers lies in the production of sufficient quantities of pure sample, as in non-native host systems protein complex assembly is often inefficient or simply incomplete, making purification and subsequent crystallisation challenging. This problem - that of complexes being present in various oligomeric states - is one that also produces difficulties (albeit for different reasons) in cryo-EM projects.

Prior to any experimental work, improvements in the cellular yield of bacterial heteromers can be achieved by carefully considering the design of the expression vector, in light of the assembly pathway of the protein complex in question. As will be discussed in detail in chapter 3, the order of genes within protein complexes is under selection to match the assembly order of protein complexes \cite{Wells2016}. It has been demonstrated experimentally that taking this fact into account can markedly increase complex assembly efficiency, and that yields of heteromers in their fully-assembled native state can be improved by using the native operon structure in expression vectors \cite{Shieh2015a, Poulsen2010}.

When purifying protein complexes there is a tradeoff between obtaining highly pure samples and ensuring that the intermolecular bonds between subunits are not disrupted. Though the diversity of methods for protein purification is bewilderingly high, in practice most methods suitable for protein complexes are variations on affinity purification. A standard, widely used protocol, (summarised in more detail by Gr{\"a}slund et al. \cite{Graslund2008}), involves first generating a hexa-histidine tagged fusion protein. This `bait' protein is then expressed in the host, preferably at endogenous levels
\footnote{Somewhat counterintuitively, increasing the abundance of a single subunit may actually decrease the yield of the native complex. To understand this, imagine a trimer, assembled linearly as follows: A-B-C. If the concentration of subunit B were to be doubled, the resulting imbalance in stoichiometry would lead to A and C being preferentially sequestered in the form of A-B and B-C dimers, which are incompatible with the original trimeric structure. The idea that differentially modulating subunit expression within complexes can be deleterious is known as the balance hypothesis \cite{Papp2003}, and will be discussed extensively in coming chapters.},
and the resulting cell extract is put through a column containing immobilised metal ions, resulting in the capture of the 6-His tagged protein, as well as anything else bound to it. Upon washing the column, the purified protein complex will be retained on the beads, and can be cleaved off the metal-coated beads in subsequent washes. Ideally, this would be ready for crystallisation, but in practice multiple washes and additional purification steps are generally required before the sample is sufficiently pure. Unfortunately, these additional steps will affect the final yield, and also risk disrupting more delicate interactions with peripheral subunits. Methods such as Dynamic Light Scattering are now frequently used to assess sample purity in a non-invasive manner.

% CITE: CITATIONS PLEASE!
The above provides a good starting point for designing a purification protocol (), but in many cases it will be necessary to tailor the process to the protein complex of interest. Depending on the orientation of subunits within the structure for example, different subunits may make better or worse bait proteins, as will N- or C-terminal histidine tags. Similarly, some complexes may be disrupted by the presence of the metal ions, in which case other beads, e.g. those coated in calmodulin, may be more suitable. Although there has been some progress towards high-throughput expression and purification pipelines, much of this work still relies on the expertise of individual structural biologists and research technicians.

Surprisingly however, the crystallisation process is still the main bottleneck in X-ray crystallography, despite having been largely automated by the development of screening robots. There have however been some important methodological developments in the crystallisation of membrane proteins, which will also be useful for many membrane complexes. In particular, an exciting new method - X-ray solvent contrast modulation - has enabled for the first time visualisation of the interaction between bulk membrane phospholipids and embedded proteins \cite{Norimatsu2017}. However, this method still requires good quality crystals, and these are obtained through trial and error - beyond a few general rules of thumb we still do not have a good understanding of how different proteins will behave under varying crystallisation conditions.

% NOTE: shorten - just say how the slightly underwhelming performance of the JPSI highlight the difficulty of expression and crystallography - this is still a major bottleneck. Actually, there is some dev of high-throughput systems. e.g "Tandem recombineering by SLIC cloning and Cre-LoxP fusion to generate multigene expression constructs for protein complex research."


\subsubsection{Diffraction pattern acquisition}
% NOTE: bragg could do with a citation probably.
Once suitable crystals have been obtained, the main hurdle has been hurdled and image acquisition can begin. In contrast to earlier steps, enormous progress has been made in this domain since William L. Bragg first demonstrated that crystals diffract X-rays in 1913 \cite{Bragg1913}. By far the most important development in this domain has been that of synchotron X-ray sources. Synchotrons are able to produce X-rays at far higher intensities than traditional sources, and as such greatly reduce the time it takes to produce diffraction patterns. Additional properties of the X-rays can also be manipulated, for example narrowing the beamline in order to focus on the best quality region of the crystal, thus improving the resolution that can be achieved from the diffraction pattern.

% NOTE: Peak "beam" energies sounds kinda silly since its a pulse, not a beam. Depending on space, it might be nice to mention how you don't have to worry about cryocooling either - Ada Yonath was one of the first to see the benefit of this in solving the structure of the ribosome. Also, it might be possible to use this for non-crystalline imaging? Is it OK to use a colon in "this becomes a non-issue:?"
More recently, X-ray free electron lasers (XFELs, fig. \ref{c1fig1}) have also begun to make an appearance in structural biology. It is hard to overstate the impact that this technology will have on the field, since XFELs are capable of producing peak beam energies approximately ten orders of magnitude greater than 3rd generation synchotrons \cite{Shi2014}, and in doing so enable a radically different approach to crystallography. The principle benefit of this additional power is that the time needed to generate a diffraction pattern is drastically reduced: from hours to femtoseconds. A crystal in the path of such high-energy photons will be vaporised, but since the diffraction pattern will be obtained faster than the sample is destroyed, this is a non-issue: a fact first noted by Neutze et al.\cite{Neutze2000}, giving rise to the term `diffraction before destruction'. However, this generates a need for a great many crystals, but in practice this too turns out not to be a problem either, since these crystals need only be a few nanometres in size. In fact, since nanoscale crystals are far easier to grow, the method also circumvents the tedious trial and error process of producing mesoscale crystals.

\begin{figure}[h]
    \includegraphics{c1_fig1_xfel_v3}
    \caption[X-ray free electron lasers]{\sffamily \textbf{X-ray free electron lasers} \\ \rmfamily These here are electron lasers, they be very nice}
    \label{c1fig1}
\end{figure}

\subsubsection{Structure determination}
% NOTE: The structure of a protein in a crystal can be determined from its electron density function. This is a function of both the phases and amplitude of the x-ray diffraction maxima.

% CITE: for stuff about the phase problem.
Interpretation of the crystal diffraction pattern required the solution of a long-standing obstacle in the early days of X-ray crystallography, known as the phase problem. The phase problem exists due to the fact that, whilst diffraction patterns capture the amplitude of diffracted photons from a crystal (seen as the intensity of spots on the photograph), the phase of those photons is lost in the process of image acquistion. Unfortunately, it is the phases of the diffracted photons, rather than their amplitudes, that carry the most information about the underlying crystal structure. The eventual solution of this problem by Max Perutz was the key to his and Kendrew's determination of the first protein structures.

Perutz's breakthrough came when he realised that a technique previously used for phasing much crystals of much smaller molecules could also work for proteins. This method, known as isomorphous replacement (IR)  \cite{Robertson1936}, incorporates a heavy metal into the crystal, but (crucially) does not significantly alter the structure of the underlying protein. As a result, the position of spots in the diffraction pattern remain almost unchanged, but subtle differences in their intensity point to the location of the heavy metals, thus providing a reference point for calculation of the X-ray phases. This method has since been followed by several others, most notably Multiple wavelength anomalous diffraction (MAD). This method operates on different principles from IR but is popular since it is limited only by the quality of the diffraction pattern provided to it. For large protein complexes, polynuclear metal clusters are often used in place of individual heavy atoms because of their particularly electron density and associated isomorphous or anomalous scattering signal \cite{Dauter2005}. This approach has recently been used to good effect in solving the structure of the notoriously difficult mediator complex \cite{Nozawa2017}.

% NOTE: Have you cited PDB yet? And will you discuss homology modelling?
As a consequence of the ever-expanding number of structures in the Protein Data Bank and the widespread availability of sequence data, it is usually possible nowadays to avoid \textit{de novo} phasing altogether. Molecular replacement makes use of the fact that closely related sequences generally have very similar folds, and therefore can be used as a template to guide brute-force solution of the phase problem. There are currently many programs that automate this process - for example, Phaser \cite{McCoy2007}, which is available within the widely used CCP4 software suite \cite{Winn2011a}.

% NOTE: Need to find the right place to discuss the difficulty of distinguishing crystal contacts from biological ones.

% NOTE: Have you repeated a resurgence in...?
\subsection{Single-particle cryo-electron microscopy}
X-ray crystallography has been, and will continue to be, an enormously useful too for investigating proteins and protein complexes. However, in recent years, a resurgence in an old technique has had an major impact on structural biology, and in particular on our ability to solve the structures of large protein complexes above 300 kDa in size (approximately 3000 residues). It's TK (unique suittability?) for larger structures is particularly useful since these often prove prohibitively difficult to crystallise, in large part due to compositional heterogeneity of the purified samples, which cryo-EM can more easily handle. The two methods are therefore highly complementary, and indeed many structures are solved to high resolution by a combination of the two - cryo-EM for the coarse grained strucutre, and X-ray crystallography for atomic resolution of individual subunits.

However, as interest in cryo-EM increases (in March 2017 the Wellcome Trust announced a £20M grant for cryo-EM equipment in several UK laboratories), there are signs that the field is gaining ground on X-ray crystallography. Two important symbolic barriers have recently been broken, with a 2016 paper in Cell simultaneously describing the structures of two homomeric complexes: isocitrate dehydrogenase and glutamate dehydrogenase. The former weighs in at just 93 kDa, and is the first single-particle cryo-EM structure of a <100 kDa complex, while the latter was resolved to 1.8Å, breaking the <2Å barrier \cite{Merk2016}. As we shall see, the remarkable technological achievements displayed in this paper and several others from the last few years have been been driven by dramatic improvements in two areas\cite{Bai2015}. Even with this rapid progress however, there is good reason to believe that further reductions in the limits of resolution are possible.

% NOTE: The first major development?
\subsubsection{Direct electron detectors}
A major development in cryo-EM came with the replacement of photographic film by digital direct electron detectors, specifically monolithic active pixel sensors (MAPS). Surprisingly, it was not until relatively recently that digital detectors came into widespread use, as for a long time they had unfavourably low detective quantum efficiencies (DQE) compared to that of film \cite{McMullan2009}. DQE is a measure of the signal to noise ratio that can be achieved relative to an ideal detector \cite{Dainty1975}, and is defined as follows:
\begin{displaymath}
    DQE = (S/N_{in})^{2}/(S/N_{out})^{2}
\end{displaymath}
Where \begin{math} S/N_{in} \end{math} and \begin{math} S/N_{out} \end{math} are the input and output signal-to-noise ratios respectively. A DQE of 1 would therefore imply that the detector was not responsible for any noise in the final image. For reference, film has a DQE of around 0.3, whereas the current state-of-the-art MAPS detectors achieve roughly twice that.

% CITE: if you can, K2 summit
Now that the DQE of MAPS detectors has surpassed that of film, several other compelling advantages can exploited. From a practical standpoint, they are significantly faster to use, since images can be viewed immediately after collection and their acquisition can be automated. More importantly however, they can be operated in counting mode, where instead of integrating the signal produced by each incident electron across all the pixels in which a charge was registered, only the pixel with the highest charge is counted \cite{McMullan2009a}. This is conceptually similar to the way in which optical microscopy techniques like PALM \cite{Betzig2006} and STORM \cite{Rust2006} achieve super-resolution images, and the company Gatan has recently developed the idea further with the introduction of a true super-resolution mode for their K2 Summit detector.

% NOTE: I think there's still some pretty substantial innacuracy here - go through it and tighten up. Also, GPCR structure released just this year in nature.
% CITE: defocus methods of proteasome @ 3Å res. AND A BRAND NEW HAEMOGLOBIN STRUCTURE!
One new technology which is begining to make an appearance is the phase-plate, which can be used to produce phase contrast during image acquisition. In order to be able to correctly distinguish different particles in the sample it is important to have good contrast in the images. Unfortunately, the method by which this contrast is currently changed relies on defocusing the image slightly: as a result, if greater the contrast required, it comes at the expense of resolution. The Volta phase-plate in theory circumvents this issue by modulating the phase directly, without affecting the focus of the image \cite{Danev2014}. Though the principle has been around for some time, it was not until recently that various practical issues were solved, enabling them to produce a 3Å structure of the 20S Proteasome, thus matching the resolution achieved by defocus methods \cite{Danev2016}.

\subsubsection{Image processing and structure determination}
A second important factor in cryo-EM's recent success has been the appearance of better image processing software, which has enabled researchers to get the most out of the concurrent developments on the hardware side. In addition to improving resolution, the emergence of electron detectors capable of producing high frame-rate videos in counting mode also had a secondary benefit, in that it enabled beam-induced motion blurring in the images to be corrected computationally, a feat that was first achieved by two groups almost simultaneuously in 2013 \cite{Bai2013, Li2013}. Since the reduction in signal quality incurred by beam-induced movement is around five-fold if uncorrected \cite{Henderson1985}, this was a highly significant breakthrough, and is now standard protocol and can be performed using the widely used RELION software \cite{Scheres2012,Scheres2014}.


% NOTE: reduce the dimensionality of the classification prob - is this correct? Still needs improving and references. Citations available from main scheres cryo-em review.
A second area in which computational improvements have been beneficial is in image classification (fig. \ref{c1fig2}). In 3D single-particle cryo-EM, individual protein complexes are fixed in random positions and orientations in the flash-frozen sample - to determine the structure, each particle captured in the imaging process must first be categorised according to its orientation. This is why asymmetrical structures are more challenging to solve. For symmetrical structures, such as the two highlighted at the begining of this section, the number of particles required in the image is usually considerably lower, since multiple axes of symmetry effectively make many of the orientations that can be observed redundant \footnote{Interestingly, in the case of two-dimensional electron crystallography (often used for membrane proteins), in which samples are generally monolayer crystals, the problem of image classification is inverted. Here, since all of the particles are fixed in the same orientation, the classification problem is trivial, and instead the challenge is to image the structure from different angles. This is achieved by tilting the plane of the sample, which become progressively more difficult as the angle increases \cite{Wisedchaisri2011}.}. This has the effect of increasing the effective number of images of the particle.
\clearpage
\begin{figure}
    \includegraphics{c1_fig2_cryoem_v2}
    \caption[Image classification in cryo-EM]{\sffamily \textbf{Image classification in cryo-EM.} \\ \rmfamily 2 Panels, one showing how individual proteins can be grouped into different orientations, a second panel showing how compositional heterogeneity can cause issues?, do something with pretty pictures and then put a gaussian noise filter over the top or something.}
    \label{c1fig2}
\end{figure}
\clearpage

Dealing with structural and compositional heterogeneity is a related problem, arising from imperfect purification or different functional states of the complex. Computational approaches for dealing with this arrived in 1998 with a maximum likelihood method for classifying with 2D images \cite{Sigworth1998}; 3D classification methods, being much more computationally intensive, did not appear till later, but are now an area of active development, since they are one of the major bottlenecks in structure determination \cite{Scheres2007,Lyumkis2013,Punjani2017}. In practice, multiple rounds of image classification and refinement are generally carried out, beginning with 2D methods and removal of low quality particles, followed by 3D methods and further polishing.

\subsection{Nuclear magnetic resonance spectroscopy}


% NOTE: These introductory paragraphs should be split into bits worrying about proteomics vs individual proteins perhaps?
\section{Non-structural characterisation of protein complexes}
Although structural methods have been and will continue to be essential for further progress in understanding the behaviour of protein complexes, by nature they only focus on a tiny fraction of the proteome at a time. If we wish to have a complete understanding of the proteome and the interactions within it, simply characterising and cataloging an ever larger number of structures is not be enough. There is a great deal of useful information that cannot be determined solely from the molecular structure of a protein complex; for example, the cellular abundance of that complex and its constituent subunits, or the weak, substoichiometric interactions that defy crystallisation. Attempts at non-structural characterisation of protein-protein interactions began in 1989 with the development of the yeast-2-hybrid assay (Y2H) \cite{Fields1989}, in which two proteins of interest are fused to a DNA binding domain and a transcriptional activator domain, allowing binary interactions (or lack thereof) between the proteins to be detected by the expression of a reporter gene. This assay has been very successful, with the original paper having been cited over 6850 times since publication; most recently, it has been used in a high-throughput manner to map the binary interaction landscape of \textit{E. coli} \cite{Rajagopala2014}, producing a map of 2,334 pairwise interactions and enabling inference of many novel protein complexes in the process.

% NOTE: Need to make it clear that mass spectrometry is useful for far more than just studies of the interactome, and indeed mass-spec will probably become to amino-acids what sequencing is to nucleic acid.
However, though simple and cost-effective, there are inherent limitations to the technique: most obviously, the use of bulky reporter domains for example risks disrupting or preventing subtle interactions between many proteins. As a result, approaches using mass spectrometry have largely superseded Y2H as the method of choice for quantitative studies of the interactome. Through technological innovation and clever experimental design, mass-spectrometry has proven to be highly versatile, and has been used for a number of different purposes, including elucidation of protein complex assembly pathways \cite{Levy2008a,Marsh2013}, investigations into the evolutionary history of complexes \cite{Wan2015}, and generation of richly detailed interactome datasets \cite{Hein2015}.


\subsection{Native mass spectrometry}
The development of soft ionisation MS techniques in the '90s was of critical importance for the study of protein complexes, as it allowed non-covalent interactions between proteins to be preserved in the gas phase. This, combined with the later development of of time-of-flight mass analysers, enabled researchers to begin studying protein complexes in their intact quaternary structure states - an approach that is commonly referred to as native MS.

Because native MS does not interfere with the intermolecular bonds between protein complex subunits, it can be used to study structural properties such as stoichiometry, compositional heterogeneity, and even dynamic processes such as protein complex assembly and disassembly.

\subsubsection{Electrospray ionisation mass spectrometry}
The ionisation method of choice for native MS is ESI, as MALDI requires the sample of interest to be mixed with a matrix, which is then ionised using lasers. This matrix is usually formed from crystallised organic acids, and these can end up disrupting or denaturing protein complexes. In contrast, ESI uses the sample as is, and ionises it by passing it through a narrow glass capillary, to which a high voltage is applied, causing the charged sample to be aerosolised as it leaves the capillary. Through evaporation of solvent from the sample and successive Coulomb fission events, the ions in this mist rapidly enter the gas phase as they move towards the mass analyser.

An important benefit of ESI over MALDI is that it produces multiply charged protein ions with regularity. This is useful when coupling ESI to tandem MS, where the protein sample is first analysed in its native state, before being fragmented and subject to a second round of mass analysis. Single charge proteins produce little useful information upon fragmentation as only a single peptide fragment will be charged, essentially wasting much of the protein. A second benefit of this tendency is that it reduces the m/z of each protein ion. This is particularly important when investigating larger proteins and protein complexes, since (historically at least) the operative range of quadrupole mass analysers is limited to about 4000 m/z. For this reason, time-of-flight mass analysers have been the mainstay of native MS for many years, since they have good resolving power and sensitivity over a much wider range than traditional quadrupole analysers. In 2005 however, Orbitrap analysers became available, and subsequent development since then has pushed the limits of their operative mass range into the tens of thousands m/z.

Another hugely important development in ESI came with the introduction of much narrower capillaries in the electrospray devices. Coupled with lower sample flow rates, this improves ionisation efficiency considerably; more importantly still, it greatly reduces the amount of sample required for each experiment. This enables analysis of proteins which are hard to purify in large quantities, or makes it possible to run experiments investigating dynamic processes such as assembly which take place over longer time scales.

\subsubsection{Applications of native mass spectrometry}
Due to its sensitive treatment of the intermolecular interactions and low sample requirements, native MS is very versatile. A common and technically straightforward use of the method is to determine the constituent parts of a particular protein complex, which can be done by analysing the protein complex in its fully assembled state, and then again with the intermolecular bonds between subunits disrupted. The identities of specific subunits can be confirmed by tandem MS, with the weight of individual subunits being determined in the first round of mass analyses, and identities being inferred from peptide fragments in the second. From this starting point, it is then possible to generate interaction maps based on the weights of peaks corresponding to different subunit combinations, as well as getting an idea of relative binding strengths.

A particularly useful application of native MS is to determine the assembly and disassembly pathways of protein complexes. This can be achieved by adding different chaotropic agents to the solution containing the intact protein complex, then observing the intermediates that are produced at different concentrations. IS THIS CORRECT?


% NOTE: Do you have to point out that it involves fragmentation?
\subsection{Cross-linking mass spectrometry}
Cross-linking mass spectrometry (XL-MS) provides information on the constituent parts of protein complexes, but can also be used to produce low-resolution structural information in the form of distance constraints between residues from different subunits. For this reason, it is particularly effective when used in combination with more established structural techniques, and as such has become an important part of an emerging, integrative approach to structural biology \cite{Stengel2012,Ward2013,VandenBedem2015}.

The best example of this approach to date is one in which XL-MS plays an important role, namely the ongoing effort to understand the structure of the nuclear pore complex (see Beck and Hurt, 2016 \cite{Beck2016}). Due to their formidable size (\textasciitilde 120 MDa in humans, compared to \textasciitilde 3.5 MDa for the ribsome) and high degree of compositional variation between species, it is often not possible to distinguish between specific subunits, many of which are paralogues; in such cases, XL-MS can, for example, provide important information about their proximity, and in doing so enable identification of ambiguous subunits within the cryo-EM electron density map \cite{Bui2013}.

\subsubsection{Chemical cross-linkers}
Much of the power of XL-MS comes from the availability of a wide variety of different cross-linkers that impose specific distance constraints on the interactions that can be probed. Similarly, the biochemical specificity of these linkers can be used to look at interactions between specific functional groups. Most commonly used are homobifunctional cross-linkers that join primary amines \cite{Leitner2016}, i.e. lysine residues or N-termini, with spacer arm lengths ranging from \textasciitilde 3Å to \textasciitilde 35Å. Plenty of alternatives exist however, allowing more nuanced approaches to the problem of interest. Heterobifunctional linkers (in constrast to homobifunctional ones) allow different groups to be targeted, for example joining amine to carboxyl (aspartate, glutamate, C-termini) groups.

By virtue of the fact that XL-MS provides information at the level of specific protein interfaces, it is especially well suited to investigating individual protein complexes and interactions. However, the power of mass-spectrometry as a whole is its range: it can be used to probe intimate mechanistic details of specific protein complexes, but also to look at entire proteomes in on a practical timescale. This is something that is beyond most structural techniques, but not affinity purification mass-spectrometry (AP-MS), which can be used to great effect at this larger scale.

% NOTE: See if you can include a warning about the pitfalls of mass spec, and why caution is particularly important. To do this, cite the Journal of proteome research paper criticising the draft proteomes by way of olfactory receptors. Also theres another one kicking about somewhere I think?

% NOTE: Good God, there's also hydrogen-deuterium exchange mass spec, which lets you see solvent accessible and inaccessible regions. You're killing me here.

% NOTE: AP-MS is starting to be combined with XL-MS apparently - would make it possible to see interaction surfaces on a proteome scale.
\subsection{Affinity-purification mass spectrometry}
In its simplest guise, AP-MS enables the identification and quantification of the interaction partners of a given protein. The general principle is as follows: a column containing beads capable of capturing your bait protein is prepared. Native cell extract (though sometimes over-expression of the protein of interest is required) is then washed over the column, leading to the capture both the bait and proteins bound to it via co-immunoprecipitation. The eluent is generally subjected to peptide fractionation, and mass-spectrometry is then used to quantify either the relative or absolute abundances of members of the purified complex. For high-throughput studies, multiple proteins are used as baits, enabling large interaction maps to be generated.

% NOTE: the paragraphs that follow are limited to those variations of the technique most pertintent to the study of protein complexes.
Though conceptually simple, AP-MS is an enormously powerful technique, and one that deserves more attention that it is going to get here. Fortunately, several reviews have been written on the topic, and I therefore direct the reader to these \cite{Oeffinger2012,Morris2014,Aebersold2016}; in particular, that by Morris and colleagues is excellent \cite{Morris2014}. For the sake of brevity, the paragraphs that follow are limited to just the most important variations of a method that has been instrumental in achieving our current understanding of the protein interactome \cite{Malovannaya2011,Hein2015,Huttlin2015,Wan2015}.

% NOTE: Perhaps briefly discuss the hein paper as a case study -
\subsubsection{Single-step versus tandem affinity purification}
Historically, there have been two main approaches to affinity purification - single-step and tandem affinity purification \cite{Rigaut1999} (TAP) (fig. \ref{c1fig3}). In the former, the bait protein is either expressed under completely endogenous conditions and captured using antibodies, or expressed with a tag such as green fluorescent protein \cite{Hubner2010} and captured using methods appropriate to the tagging system. In contrast, TAP makes use of a specific TAP-tag, which consists of a protein A domain and a calmodulin binding peptide, linked by a tobacco etch virus protease cleavage site. This tag enables a two-step purification procedure, first by capture of protein A on immunoglobulin-G beads, followed by protease cleavage and recapture on calmodulin beads. This second washing step allows for very stringent purification of complexes.

\begin{figure}[h]
    \includegraphics{c1_fig3_apms}
    \caption[Common approaches to affinity-purification mass spectrometry]{\sffamily \textbf{Common approaches to affinity-purification mass spectrometry.} \\ \rmfamily 2 panels showing AP-MS, single step with endogenous and tagged, tandem affinity, and if there's space or you can think of a nice way of representing it, affinity enrichment.}
    \label{c1fig3}
\end{figure}

TAP neccesarily requires tagging of the bait protein, but in the single-step procedure it is possible to avoid this if desired. The are some straightforward trade-offs to consider when deciding whether to use endogenous or tagged proteins: For non-tagged, endogenously expressed baits, you have the benefit of capturing the protein in its native state. However, this comes at the substantial cost (both in time and money) of having to raise specific antibodies against the protein. Furthermore, there are difficult issues associated with cross-reactivity and specificity when using antibodies, particularly in studies where multiple proteins are being targeted. Though there are methods that attempt to deal with these issues (most notably QUICK \cite{Selbach2006}), in the majority of large-scale studies tagging of bait proteins is likely to be more practical.

% NOTE: Can you cite some papers spanning the range of time from first TAP studies to presentish? Also, don't like that last sentence - it's too presumptuous.
Whether using endogenous or tagged proteins, there are some fairly compelling advantges to using single-step procedures in general over TAP. TAP has been an important technique for almost 20 years, but its raison d'être was to remove as many possible contaminants or non-specific interactors from the purified protein complex. This was necessary in the early days of mass spectrometry, since it was not possible to quantify protein abundances, and thus contaminants in the sample would be erroneously annotated as members of the protein complex. However, there have been enormous improvements in the sensitivity of mass spectrometers since TAP was first described. With these improvements, particularly in the area of label-free quantification, there has been a growing appreciation of the importance of weak, non-obligate interactions between proteins \cite{Perkins2010a,Hein2015}. By design, TAP removes these weak interactors, and thus its utility is becoming increasingly restricted, at least for studies of the interactome. Therefore, unless equipment is an issue, single-step procedures combined with accurate quantification should be considered preferable to TAP where possible, particularly for large-scale studies.

\subsubsection{Quantification of protein abundances}
The emergence of quantitative mass spectrometry, via both label based and label free-methods, has had a transformative effect on the field of proteomics. For our purposes, the principal benefit arising from the ability to quantify protein abundances is that it allows the stoichiometry of protein complexes to be determined. This is essential for distinguishing obligate interactions from transient ones, and - more generally - for providing a complete characterisation of the complex. The difficulty in using mass spectrometry as a quantitative tool is that, whilst the location of peaks on the spectrum allows identification of peptides, peak intensity alone is not sufficient to determine abundance. Label-based methods such as SILAC \cite{Ong2002a} and iTRAQ \cite{Ross2004} (amongst others \cite{Gygi1999,Thompson2003}) allow for either relative or absolute quantification (through metabolic incorporation of amino-acid isoptopes in the case of SILAC, and N-terminal isobaric tags in iTRAQ).

A significant drawback to label-based methods is their cost, which can be prohibitive. An alternative approach is label-free quantification (LFQ), which in general rely on either spectral counting \cite{Liu2004,Zybailov2005} or peak intensity-based algorithms. Spectral counting is a conceptually simple, semi-quantitative approach, and has been widely used (possibly abused \cite{Lundgren2010} - `semi' being the operative word). Intensity-based algorithms undoubtedly offer more accurate quantification; for the interested reader, comparative analyses and reviews of several available methods are available \cite{Nahnsen2013,Fabre2014}. One recently developed algorithm of note that has been enthusiastically received by the community is MaxLFQ \cite{Cox2014}, which is available as part of the larger MaxQuant software package \cite{Cox2008}.

% NOTE: They definitely (?) use this in the Hein paper, but what about the Wan one? Seems they came out at around the same time, so perhaps they arrived at a similar method more or less independently.

% NOTE: How to put it all together? AE-MS is way too specific - has only been cited a few times, though it is very new. Perhaps talk about fractionation instead. Fractionation vs pre-fractionation?
% \subsubsection{Affinity enrichment mass spectrometry}
% A recent development in the field of AP-MS - one with particular importance for the study of protein complexes - is that of affinity enrichment mass spec (AE-MS) \cite{Keilhauer2015}. Rather than being a technological break-through in any one area, this instead combines several of the elements we have been looking at in this section.

% NOTE: Discuss the importance of quantification, particularly relative vs absolute, SILAC vs iBAQ. This is the place to discuss the pitfalls of mass spectrometry. Need to at least skim those spectral counting papers. Also go through the affinity enrichment paper intro with a fine-tooth comb for some useful references for the next section. https://www.coursera.org/learn/experimental-methods/lecture/n7Z1J/lecture-3-quantification-in-proteomics

% NOTE: Bleurgh
\subsubsection{Inferring protein complexes from interaction networks}
Once processed, the data returned from high-throughput techniques such as those we have been discussing are often presented in the form of a graph, in which different proteins are represented as vertices and interactions between them as edges, weighted by whatever measure is appropriate. The challenge here is to infer distinct protein complexes from this set of interaction data. As might be expected given the generalisable nature of this problem, a great number of methods exist for tackling it, both specific to protein-protein interactions and otherwise. The majority of succesful methods rely on forms of graph clustering, building on the valid assumption that protein complexes should form densely connected cliques.

One particularly elegant and widely applicable example in this category is the Markov Cluster algorithm \cite{VanDongen2000} (MCL), which is based on the principle that if you take a random walk through a graph, the strongest clusters will be those nodes groups of vertices that you tend to stay in for longest. Despite its age, MCL still competitive \cite{Li2010}, and has been used in a number of excellent interactome papers since its release \cite{Krogan2006,Wan2015}. Other notable mentions include COACH and ClusterOne \cite{Wu2009,Nepusz2012}, the latter of which is one of the few alternatives to give MCL a serious run for it's money.

\subsection{Super-resolution microscopy for structural biology}
A common feature of all of the above methods is their inability to observe proteins in vivo. Super-resolution microscopy, so named because it breaks the diffraction limit for light microscopy described by Ernst Abbe, revolutionised the field of microscopy when PALM and STORM arrived. The earliest efforts include palm and storm, and these were

\subsubsection{Stochastic super-resolution}

% NOTE: The interest in computational prediction is such that twop competitions have been running for several years...
\section{Computational prediction of protein complex structure}
% NOTE: Perhaps start with a very short paragraph talking about how computational analysis is integral to structure characterisation, both for structural methods and non-structuralm, inferring complexes from interaction data (spoke affinity etc etc models.) Then lead into the prediction from sequence.
Frequently, experimental methods for determing protein complex structure are not possible, or tellingly, are simply no longer the best use of a researcher's time and money. Prediction of protein structure from sequence, first prophesised by Anfinsen in 1973\cite{Anfinsen1973}, has been a long-standing challenge in biology which has until recently been impossible in practice, for want of both sequence data and computational power. However, the genomic era has seen exponential increases in both of these areas, along with a similar expansion in the number of experimentally determined protein structures. Concomitantly, there has been a significant improvement in our ability to predict structures computationally. Indeed, the interest in modelling of structure is such that two important competitions - CASP (Critical Assessment of protein Structure Prediction \cite{Moult1995}) and CAPRI (Critical Assessment of PRediction of Interactions \cite{Janin2003}) - have been now been running for many years. These competitions are now an integral part of the community, providing much-needed benchmarks and progress reports that enable both users and developers to keep track of the rapidly changing state of the art.

Broadly speaking, the field of protein structure modelling can be separated into two overlapping subgroups, separated more by philosophical standpoints than genuine differences. Older, and currently more practical, are top-down approaches based on the use of templates for the structures being modelled,  which are typically selected based on close sequence similarity with the target protein or complex. In contrast, there is equally great interest in prediction of structure from first principles - an approach that is exemplified by the field of molecular dynamics. Template-based modelling (TBM) and molecular dynamics (MD) represent two opposite sides of the protein modelling community, but in practice there is a great degree of overlap between the two, and most of the methods the methods that I will describe below borrow elements from both, as is the case in molecular docking, which aims to model protein-protein interfaces.
%
% Although enormously useful, homology modelling is limited by the requirement for template structures that are a close sequence match to the protein of interest. This limitation is apparent in the case of membrane proteins and other difficult-to-solve structures, which due to their relative scarcity in the PDB are also less amenable to homology modelling \cite{Reddy2006}. In addition, without the aid of experimental data or additional computational tools, homology modelling alone is less able to predict protein-protein interactions, and thus protein complexes in general. For this reason, there is still great interest in ab initio, template-free prediction of structures, as well as methods for predicting protein-protein interactions \cite{Bonvin2013,Moult2016}.

\subsection{Top-down modelling of protein complex structure}
The extent of the improvement in predictive power is such that, for sequences with close sequence similarity to known structures, it is usually possible to produce structures that are within a few ångströms of the experimentally determined version, as measured by root-mean-square-deviation of residue distances and other metrics \cite{Haas2013,Moult2016}. This process is known as template-based modelling (TBM), and nowadays is routinely used to facilitate experimental structure determination of single protein chains.

% NOTE: Can you find any better references in the Szilagyi review?
For complete protein complexes, regardless of whether the structures of individual subunits are already known, it is often possible to reach a realistic approximation of the correct protein complex structure by a combination of homology modelling and molecular docking. Though most applicable for investigating protein-ligand interactions, molecular docking combined with homology modelling is becoming increasingly viable for protein-protein interactions, as evidenced by numerous recent studies and the results from the CASP and CAPRI competitions \cite{Jiang2013,Rajapaksha2014,Agostino2016,Lensink2016}.

\subsubsection{Template-based modelling}
TBM is based on the principle that the degree of sequence divergence in homologous proteins is closely related to their structural similarity \cite{Chothia1986}. Once a suitable template is found - typically with at least 40\% sequence identity with the target protein - the sequences are aligned and conserved regions are used to map fragments of the target onto the template structure. This is followed by replacement of the loop regions (which tend to be less well conserved) and additional refinement procedures.

There are numerous methods based on extensions of this basic protocol that enable modelling of complete protein complexes, in addition to individual subunits \cite{Chen2008,Tuncbag2011,Guerler2013}. One important and widely used strategy for template identification is threading, or dimeric threading, in the case of modelling complexes \cite{Bowie1991,Lu2002}. Threading differs slightly from approaches based solely on sequence homology, in that it relies more on fold recognition than sequence similarity - this is assessed by a scoring function - the template that is eventually selected is the one which minimises this function. As a general rule, threading is used when the target sequence has particularly low sequence similarity with other known proteins, but in practice, most modern software takes these decisions out of the hands of the user. For a much more comprehensive overview of the software and underlying strategies behind TBM, readers should see the recent review on the topic by Szilagyi and Zhang \cite{Szilagyi2014}.

% NOTE: de novo docking and template driven docking.
\subsubsection{Prediction of protein-protein interfaces}
Purely template-based methods (though in practice all use a combination of approaches) work by mapping the sequence of the target proteins of interest onto a template protein complex, without explicitly modelling the interface until later refinement steps. In contrast, molecular docking begins with subunits in their monomeric form, and aims to find the interface that minimises the global energy landscape of the bound proteins. This is done by sampling the conformational space of the two proteins with respect to eachother and scoring the different interfaces that can be formed between them \cite{Huang2014}.

These two steps have been solved in a number of ways. The sampling problem is computationally very intensive, since two chains moving in three dimensions results in six degrees of freedom, to say nothing of allowing sidechain and backbone movements. Thus, intially at least, virtually all methods assume the proteins of interest to be rigid bodies, and fix the orientantion of one protein with respect to the other. This reduces the complexity of the sampling problem greatly. A variety of approaches to the problem exist, beginning with fast Fourier transforms \cite{Katchalski-Katzir1992}, which were the first to make molecular docking possible. Other approaches have subsequently been developed, including Monte Carlo search \cite{Hart1992} and normal mode analysis \cite{Zacharias2005}. Monte Carlo search is of note because of its use in one of the most popular and successful docking programs - RosettaDock \cite{Lyskov2008,Zhang2015}.

The second aspect of docking is scoring of interfaces, which is itself a non-trivial problem. As in the wider field of structure modelling, which encompasses docking, solutions to this problem tend to take the form of either physical or empirical and knowledge-based methods. In the former camp, force field scoring functions \cite{Kynast2016} are used to model the energy of the system in a given conformation, and typically involve a large number of parameters relating to attributes such as Van der Waals interactions and intramolecular strain energies. The latter camp consists of conceptually simpler techniques including counting of intermolecular contacts \cite{Bohm1998} and scoring based on prior knowledge of statistically likely interactions \cite{Sasse2017}, gleaned from sources such as the PDB.

% NOTE: There is considerable diversity in the methodological details underlying the software, but some of the most popular and succesful, as judged by CAPRI are ... which use ... and are therefore good for ...
There are several software tools available that are designed to predict intermolecular residue contacts that are formed between two proteins (often referred to as `docking'), with RosettaDock \cite{Lyskov2008} and HADDOCK \cite{Dominguez2003,VanZundert2016} being amongst the most popular.

\subsection{De novo structure prediction}
The docking methods described above are contingent on having structures available for the proteins whose interactions you are trying to model. However, is often the case that there is no experimentally solved structure or suitable template for homology modelling. In the past, this would have meant that the best one could do would be to try and predict secondary structure regions and infer the presence of binding domains from homologous sequences using tools such as JPred4 \cite{Drozdetskiy2015} (for secondary structure) or databases such as PFAM and Uniprot \cite{Finn2016,Consortium2017} for domain predictions and functional annotations. However, there are promising signs that prediction of protein structure from first principles alone will one day be possible, and in other areas important work is being done that circumvents the substantial difficulties entailed by truly ab initio methods.

\subsubsection{Using protein coevolution to infer intermolecular contacts}
Using the evolutionary sequence record to inform structure prediction is an idea that has been around for at least 20 years \cite{Altschuh1987}, but has been prevented from becoming practical by the fact that it is very challenging to distinguish coevolving sites that indicate true amino acid contacts from those that are transitive. For example: if residues A and B are in contact, and residues B and C are in contact, then A and C may also show a strong coevolutionary signal, despite interacting via an intermediary residue. Over the entire protein sequence, this blurring of coevolutionary signal is sufficient to prevent meaningful structure prediction. The major breakthrough in tackling this problem was achieved with the developement of an algorithm named Direct Coupling Analysis \cite{Weigt2009,Lunt2010}, which extends Shannon's concept of mutual information \cite{Shannon1948} and enables direct and indirect residue contacts to be distinguished from eachother. This was then implemented in a more generally applicable and user-friendly format primarily by Deborah Marks, enabling the method's widespead use in the structure-prediction community \cite{Marks2011,Marks2012,Hopf2014}.

% NOTE: Need a citation or two for the stuff about heteromeric interactions arising from gene duplications.
Though originally used for single protein structures, is equally applicable to protein complexes, as intermolecular contacts are subject to many of the same coevolutionary pressures as intramolecular ones. Thus, the EVcouplings method has recently been successfully applied to protein complexes \cite{Hopf2014}. Out of a set of 82 protein complexes with unsolved structures, 32 had a sufficiently good sequence record as to be able to predict the entire complex de novo, whereas others were sufficient to predict intermolecular contacts, but not the entire structure. Unfortunately, a major limitation of this technique is identification of homomeric contacts, since without additional information they cannot be distinguished  from intramolecular interactions. A releated issue is that many nominally heteromeric interactions arise from homomeric interactions between genes which have undergone duplication and and subsequent genetic drift \cite{Wagner2001,Wagner2003,Fokkens2012}. In such cases, it may not be possible to acquire a sufficient number of sequences for structure/interaction prediction, particularly if the proteins in question have diverged recently.

\subsubsection{Molecular Dynamics}
Saturation of structure and sequence space is a long way off, as is made clear from recent studies sampling viral and prokaryotic genomes \cite{Brum2016,Shi2016,Mukherjee2017}. As such, a substantial proportion of the protein universe is beyond the reach of either TBM or methods using information from the evolutionary sequence record. Molecular dynamics is a simulation method that models the behaviour of all the atoms or molecules in a dynamical system. This is typically achieved, as in the force field method highlighted earlier, by numerically solving Newton's equations of motion for the entire system of particles. Being rooted in basic physical principles, and unlike most of the other techniques we have discussed, MD is enormously versatile, and is used widely in other fields outside of biology \cite{Hu2013,Rapaport2014,Shao2015}. From our perspective, much of its power lies in its utility across multiple cellular scales - it has been used to study processes ranging from the molecular details of ligand binding to the assembly of virus capsids \cite{Buch2011,Zhao2013}.

% NOTE: why are AMBER and CHARMM particularl good for protein complexes
Though ab initio MD methods exist which explicitly include the electronic configurations as parameters in the model, these are not practical for systems beyond a few atoms in size and across timescales of more than a few picoseconds. For biological macromolecules on the scale of protein complexes, empirical force fields are used instead. These describe an approximation of the potential energy of the system through a function with the general form \begin{math} U(r_{1}, r_{2}, r_{3}, ..., r_{N}) \end{math}, where \begin{math} R = \{r_{1}, ..., r_{n}\} \end{math} describes the coordinates of the atoms in the system. It is this same potential energy function that is minimised in molecular docking simulations.
% NOTE: Can you find a typical expression for a system such as AMBER?
\begin{align*}
    U &= E_{bonding} + E_{non-bonding}\\
    E_{bonding} &= E_{bonds} + E_{angles} + E_{torsions}\\
    E_{non-bonding} &= E_{electrostatic} + E_{Van\ der\ Waals}
\end{align*}

Each of the energy terms in the above equations are themselves functions of the atom coordinates and empirically determined constraints such as bond strength, molecular weight and so on. Though computationally more efficient for ignoring the electronic degrees of freedom, a drawback to force field methods is that large changes such as such as the making or breaking of covalent bonds cannot be modelled. However, in the case of protein complexes this is rarely a problem. Many different force fields have been developed for a variety of purposes, but for studying protein-protein interactions AMBER \cite{Salomon-Ferrer2013,Amber2017} (Assisted Model Building and Energy Refinement) and CHARMM \cite{Brooks2009} (Chemistry at HARvard Molecular Mechanics) are particularly relevant. The dedicated structure prediction software FoldX \cite{Schymkowitz2005} is also reliant on its own force field, with similar construction to the general one described above.

At present, computational power is only just beginning to reach the level at which the assembly of protein complexes can be studied. The two major obstacles barring further progress are the size of proteins, and the timescales on which assembly takes place. Currently, only small proteins or protomers can be modelled, and even then only across very short timescales. This is a problem, since folding and protein complex assembly takes place over timescales of microseconds to minutes. However, a recent exciting study by Plattner and colleagues \cite{Plattner2017} has had some success using hidden markov models in combination with thousands of MD simulations starting from different points, selected so as to maximise the efficiency of conformational space exploration. This has enabled quantitatively accurate modelling of the bacterial ribonuclease barnase in complex with its inhibitor barstar. Associated, dissociated and transition states were observed across 2 milliseconds of modelling time, and predicted thermodynamic parameters were in strong agreement with those obtained from independent experimental results.

\subsection{Protein complex structure databases and repositories}
You can refer back to Margarett Dayhoff and the first database repositories! Yay! Nice to come almost full circle.

% NOTE: Things there wasn't space for - prediction of contacts in disordered proteins.
\section{Discussion}
As we have seen, the study of protein complexes is currently undergoing a sea-change, brought about by recent breakthroughs in structural biology, the emergence of mass spectrometry as a quantitative tool, and ongoing developments in computational techniques. Unfortunately, I have had to ignore many other tools that have played a part along the way. Most glaring amongst these omissions is nuclear magnetic resonance spectroscopy, which is responsible for many of the structures currently available in the PDB. Traditionally, it has been used for smaller proteins due to the inherent difficulty in collecting data from large proteins with short relaxation times. Having said that, protein complexes are by no means beyond reach, as is clear from a recent selection presented by Huang and Kalodimos \cite{Huang2017}.

Other notable techniques that have been missed out include small-angle x-ray scattering \cite{Mertens2010}, which produces low resolution structural information, and hydrogen-deuterium exchange MS \cite{Zhang2015b}, which can be used to detect structural or chemical changes such as ligand binding or phosphorylation \cite{Wang2009}. Both of these tools, and indeed all of those we have discussed, are now widely used in the relatively new field of integrative structural biology.

\subsubsection{Integrative structural biology}
In the same way as parallel improvements in electron detectors and image-processing software have had a synergistic effect in cryo-EM, so too have many other advances across the whole field of structural biology. To give a few examples: homology modelling has enabled much faster processing of diffraction patterns and electron density maps, improved purification techniques benefit essentially all of the non-computational techniques we have discussed, advances in MAPS detectors in cryo-EM will carry over to XFELs, and so on. This leaking of technologies across fields has enabled the rise of integrative structural biology, which seems certain to become the dominant approach over the course of the next decade. Early-career structural biologists today can no longer be content to specialise in one method or the other, but instead should be familiar with all of the topics covered in this chapter \cite{Shi2014,Cassiday2014}.

Many of the most impressive structures to have been published in the past couple of years have been the product of a combination of methods, for example: the transcribing mammalian polII complex \cite{Bernecky2016}, the nuclear pore complex mRNA export platform \cite{Fernandez-Martinez2016}, and the Mediator complex \cite{Tsai2017}. In addition to their integrative approaches, a second common feature of all of these papers is their focus on mechanistic descriptions of function or assembly. However, as more researchers begin to use these combined approaches, one of the difficulties they face will be in utilising data and constraints acquired from different methods to build unified models. Further progress in the field will require the development of computational platforms and resources for integrating diverse data sources, in addition to the work that is already underway \cite{Russel2012,Schneidman-Duhovny2014,Sali2015}.

\subsubsection{Moving beyond an inventory of parts}
As is clear from the shift from purely descriptive studies to mechanistic ones, there is more to proteins and protein complexes than simple descriptions of structure. Likewise, the proteome is more than just the set of proteins that make it up. In the former case, there is much to be gained from understanding the assembly process of protein complexes. It is now well established that this occurs along ordered, thermodynamically favourable pathways, and papers studying the process have continued to be published since this fact was first demonstrated \cite{Levy2008a,Marsh2013,Appolaire2014,Macek2017,Mallik2017}.

On a larger scale, inventive use of mass spectrometry is enabling rapid improvement in our understanding of how individual protein complexes fit into the wider proteome. In a stand-out study from the group of Matthias Mann \cite{Hein2015}, the proteome of HeLa cells was quantified in such a way as to accurately capture interaction stoichiometries and global cellular abundances. Although not enormously surprising, the results from this work conclusively demonstrated for the first time that the large majority of interactions, though important, are weak and occur at far from equal ratios. In contrast, stable complexes formed from interactions with stoichiometric ratios on the order of 1:1 are significantly rarer, but nonetheless highly connected through these weaker interactions.

% CITE: new haemoglobin volta phase plate
\subsubsection{Where next with protein complexes?}
The long term objective of the techniques we have discussed in this review is to give us a complete and unified understanding of the cellular proteome, in both its constituent parts and its behaviour at large scale. The progress we have made towards this aim would scarcely have been imaginable to the researchers who first began studying proteins in the 1950s, there is no reason to suspect that the next 50 years will not see even greater progress. In many of the fields I have discussed there are emerging technologies that will be revolutionary - nowhere more so than with the development of XFELs and serial femtosecond x-ray crystallography. It will be fascinating to see the new studies that this technology enables, or if cryo-EM will be able to continue along its current trajectory to overtake crystallography as the go-to method in structural biology.

In the field of mass spectrometry, although there are no obviously disruptive technologies on the horizon (though nobody expects the Spanish inquisition), continuing improvements in the sensitivity and accuracy of detectors are assured. One of the most pressing questions in biology today is the extent to which alternative splicing affects the size of the interactome, but currently we struggle to differentiate between different isoforms of the same protein. This, along with peptide discrimination in general, is a critical issue which needs addressing, and one which has been made its presence felt in our first serious attempts to map the human proteome \cite{Kim2014,Wilhelm2014,Ezkurdia2014}. Nonetheless, the inherent versatility of the method across different cellular scales ensures the field's relevance in the decades to come, particularly as we move towards single-cell biology \cite{Macaulay2017}.

Last, but by no means least, computational modelling of proteins continues to go from strength to strength. Though not a panacea for the difficult challenges we still face in structural biology, as the diversity of sequences and structures increases, so too will our ability to leverage computational power to fill in the gaps through homology modelling. Molecular dynamics too is finally approaching a point at which we can use it to study real-time processes of complex assembly. What is more, if quantum computing becomes a reality outside of specialised laboratories, then this field (not to mention everything else) will look unimaginably different in subsequent decades.

\printbibliography

\end{document}
